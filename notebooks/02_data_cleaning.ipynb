{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34746cc",
   "metadata": {},
   "source": [
    "# Data Cleaning - Streamly Case Study\n",
    "\n",
    "## Overview\n",
    "This notebook documents the data cleaning process for the Streamly recommendation system. I extract three raw CSV files, identify data quality issues, and produce cleaned datasets ready for database ingestion.\n",
    "\n",
    "## Cleaning Objectives\n",
    "1. Handle missing values appropriately\n",
    "2. Convert data types to correct formats\n",
    "3. Remove duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c824aaeb-f37c-47fd-9f83-608586b7cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fea2266-745b-45b5-8b89-a03ad9223e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"../data/results.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c872aa4e-a323-4848-b318-4347fb5e04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns = [\n",
    "    \"timestamp\", \"client_id\", \"email\", \"invoice_id\", \"period\",\n",
    "    \"product\", \"entry_type\", \"environment\", \"baseline_value\",\n",
    "    \"actual_value\", \"currency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32ff197d-87ad-4b33-9409-e025bbe0313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = pd.read_csv(\"../data/profiles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe9c64",
   "metadata": {},
   "source": [
    "## Data Cleaning Steps\n",
    "\n",
    "### 1. Results Dataset (results.csv)\n",
    "**Issues Identified:**\n",
    "- Timestamp fields stored as strings - need datetime conversion\n",
    "- Period field contains invalid date formats - `pd.to_datetime(..., errors='coerce')` handles conversion failures\n",
    "- Monetary values (baseline_value, actual_value) stored as strings - converted to numeric with coercion\n",
    "- Missing product and entry_type values - filled with \"Unknown\" placeholder to maintain referential integrity\n",
    "- Inconsistent case and whitespace in categorical fields\n",
    "\n",
    "**Actions Taken:**\n",
    "- Convert timestamp and period columns to datetime format\n",
    "- Convert baseline_value and actual_value to float for calculations\n",
    "- Fill missing categorical values with \"Unknown\" to preserve row count\n",
    "- Maintain client_id and email for account relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10ea2d17-9fd2-4bed-bb2c-9fd2e2397ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_csv(\"../data/titles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d01a14",
   "metadata": {},
   "source": [
    "### 2. Profiles Dataset (profiles.csv)\n",
    "**Issues Identified:**\n",
    "- Missing values in age_band, preferred_language, and preferences columns\n",
    "- Duplicate profile entries across accounts\n",
    "\n",
    "\n",
    "**Actions Taken:**\n",
    "- Keep missing values in preference fields (NULL) to allow conditional filtering in recommendations\n",
    "- Remove exact duplicate rows using `drop_duplicates()`\n",
    "\n",
    "\n",
    "### 3. Titles Dataset (titles.csv)\n",
    "**Issues Identified:**\n",
    "- Missing values in some columns\n",
    "\n",
    "\n",
    "**Actions Taken:**\n",
    "- Keep missing regions and languages (NULL) - will be filtered during recommendation queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "996c8e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiles: dropped 0 exact duplicate rows (359 -> 359)\n",
      "Titles: preserved missing values for region/language where present\n",
      "Profiles: dropped 0 exact duplicate rows (1000 -> 1000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    # Profiles: keep preference-related missing values\n",
    "    if 'profiles' in globals():\n",
    "        pref_cols = ['age_band', 'preferred_language', 'preferences']\n",
    "        for c in pref_cols:\n",
    "            if c in profiles.columns:\n",
    "                # ensure missing values remain as NaN/None (no imputation)\n",
    "                profiles[c] = profiles[c].where(profiles[c].notna(), None)\n",
    "\n",
    "        # Remove exact duplicate profile rows\n",
    "        prof_before = len(profiles)\n",
    "        profiles = profiles.drop_duplicates()\n",
    "        prof_after = len(profiles)\n",
    "        print(f\"Profiles: dropped {prof_before - prof_after} exact duplicate rows ({prof_before} -> {prof_after})\")\n",
    "    else:\n",
    "        print('Profiles DataFrame not found in notebook namespace; ensure it is loaded earlier.')\n",
    "except Exception as e:\n",
    "    print('Error while cleaning profiles:', e)\n",
    "\n",
    "try:\n",
    "    # Titles: explicitly preserve missing 'region' and 'language' \n",
    "    if 'titles' in globals():\n",
    "        for c in ['region', 'language']:\n",
    "            if c in titles.columns:\n",
    "                titles[c] = titles[c].where(titles[c].notna(), None)\n",
    "        print('Titles: preserved missing values for region/language where present')\n",
    "        \n",
    "        titles_before = len(titles)\n",
    "        titles = titles.drop_duplicates()\n",
    "        titles_after = len(titles)\n",
    "        print(f\"Profiles: dropped {titles_before - titles_after} exact duplicate rows ({titles_after} -> {titles_after})\")\n",
    "    else:\n",
    "        print('Titles DataFrame not found in notebook namespace; ensure it is loaded earlier.')\n",
    "except Exception as e:\n",
    "    print('Error while processing titles:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbbf714e-d7e8-4714-a291-53e217fa6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean results.csv ---\n",
    "results[\"timestamp\"] = pd.to_datetime(results[\"timestamp\"], errors=\"coerce\")\n",
    "results[\"period\"] = pd.to_datetime(results[\"period\"], errors=\"coerce\")\n",
    "\n",
    "results[\"baseline_value\"] = pd.to_numeric(results[\"baseline_value\"], errors=\"coerce\")\n",
    "results[\"actual_value\"] = pd.to_numeric(results[\"actual_value\"], errors=\"coerce\")\n",
    "\n",
    "results[\"product\"] = results[\"product\"].fillna(\"Unknown\")\n",
    "results[\"entry_type\"] = results[\"entry_type\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f0a306-1c69-4f27-8acc-fdc653bee3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved cleaned files to ../data/\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned datasets\n",
    "results.to_csv(\"../data/results_clean.csv\", index=False)\n",
    "profiles.to_csv(\"../data/profiles_clean.csv\", index=False)\n",
    "titles.to_csv(\"../data/titles_clean.csv\", index=False)\n",
    "\n",
    "print('\\nSaved cleaned files to ../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de388ac-3f26-436d-ab52-636fdfd49a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
